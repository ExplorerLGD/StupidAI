{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MYDQN",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ExplorerLGD/StupidAI/blob/master/MYDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwu6mttxsNkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY0voBZduJ4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIzZeWR_0zIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper Parameters\n",
        "BATCH_SIZE = 128\n",
        "LR = 0.01                   # learning rate\n",
        "EPSILON = 0.9               # greedy policy\n",
        "\n",
        "TARGET_REPLACE_ITER = 100   # target update frequency\n",
        "MEMORY_CAPACITY = 2000\n",
        "\n",
        "GAMMA = 0.9\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "TARGET_UPDATE = 10\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self,):\n",
        "    super(Net,self).__init__()\n",
        "    self.fc1=nn.Linear(1,50)\n",
        "    self.fc1.weight.data.normal_(0,0.1)\n",
        "    self.out=nn.Linear(50,1)\n",
        "    self.fc1.weight.data.normal_(0,0.1)\n",
        "  def forward(self,x):\n",
        "    x=self.fc1(x)\n",
        "    x=F.relu(x)\n",
        "    actions_value=self.out(x)\n",
        "    return actions_value\n",
        "  \n",
        "\n",
        "policy_net,target_net=Net(),Net()\n",
        "steps_done=0\n",
        "memory = ReplayMemory(10000)\n",
        "optimizer = torch.optim.Adam(policy_net.parameters(), lr=LR)\n",
        "loss_func = nn.MSELoss()\n",
        "n_actions =2\n",
        "\n",
        "def select_action(state):\n",
        "  global steps_done\n",
        "  sample=random.random()\n",
        "  eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
        "  steps_done += 1\n",
        "  if sample > eps_threshold:\n",
        "      with torch.no_grad():\n",
        "          # t.max(1) will return largest column value of each row.\n",
        "          # second column on max result is index of where max element was\n",
        "          # found, so we pick action with the larger expected reward.\n",
        "          return policy_net(state).max(1)[1].view(1, 1)\n",
        "  else:\n",
        "      return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)  \n",
        "    \n",
        "def optimize_model():\n",
        "  if len(memory)<BATCH_SIZE:\n",
        "    return\n",
        "  transitions=memory.sample(BATCH_SIZE)\n",
        "  batch = Transition(*zip(*transitions))\n",
        "  \n",
        "  non_final_mask=torch.tensor(tuple(map(lambda s:s is not None,batch.next_state)),device=device,dtype=torch.uint8)\n",
        "  non_final_next_states=torch.cat([s for s in batch.next_state if s is not None])\n",
        "  state_batch=torch.cat(batch.state)\n",
        "  action_batch=torch.cat(batch.action)\n",
        "  reward_batch=torch.cat(batch.reward)\n",
        "  \n",
        "  state_action_values=policy_net(state_batch).gather(1,action_batch)\n",
        "  next_state_value=torch.zeros(BATCH_SIZE,device=device)\n",
        "  next_state_value[non_final_mask]=target_net(non_final_next_states).max(1)[0].detach()\n",
        "  expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "  \n",
        "  loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  for param in policy_net.parameters():\n",
        "    param.grad.data.clamp_(-1,1)\n",
        "  optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTHAmTtyrICz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Env():\n",
        "  def __init__(self,):\n",
        "    self.num=0\n",
        "    \n",
        "  def get_num(self):\n",
        "    num=torch.randint(0,100,[1])\n",
        "    self.num=num\n",
        "    return num\n",
        "  def step(self,x):\n",
        "    result=abs(x-self.num)\n",
        "    done=0\n",
        "    info=0\n",
        "    return result,result,done,info\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co3_Z3WkqKX8",
        "colab_type": "code",
        "outputId": "99ef3552-1019-4831-8281-b3cba874ed84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "env=Env()\n",
        "num_episodes = 50\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    # Initialize the environment and state\n",
        "    #env.reset()\n",
        "    #state=(编不下去了)\n",
        "    for t in count():\n",
        "        # Select and perform an action\n",
        "        action = select_action(state)\n",
        "        _, reward, done, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        # Observe new state\n",
        "        last_screen = current_screen\n",
        "        current_screen = get_screen()\n",
        "        if not done:\n",
        "            next_state = current_screen - last_screen\n",
        "        else:\n",
        "            next_state = None\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the target network)\n",
        "        optimize_model()\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "\n",
        "            break\n",
        "    # Update the target network, copying all weights and biases in DQN\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "print('Complete')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-04332b749d13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Select and perform an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'state' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CHUUiYdl0bq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=torch.tensor([2,3,2,4])\n",
        "gather=test.gather(0,torch.tensor([0,1]))\n",
        "print(gather)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}